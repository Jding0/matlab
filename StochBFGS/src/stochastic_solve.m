% stochastic_solve is the template optimization method for solving
%     min  \sum_i f_i(x)
% where gradients and Hessian-vector products are subsampled using partial sums of f_i.
% It initiates by calling method_boot(...).
% At each iteration, a step direction is generated by calling d= descent_method(...).
% Copyright (c) 2014.  Robert Gower.
function [out] =  stochastic_solve(x0, f_eval, g_eval, Hess_opt, boot_method, descent_method,   opts)
% Boot the descent method
if(~isempty(boot_method))
    boot_method(x0,f_eval, g_eval, Hess_opt,opts);
end
% Boot the gradient type method
if(strcmp(opts.grad_type,'SGD'))
    opts =calibrate_SGD(x0,f_eval, g_eval, Hess_opt,opts,boot_method, descent_method);
elseif(strcmp(opts.grad_type,'SVRG'))
    % setting up SVRG steps
    boot_SVRG;
end

%% Get best step size? TIME CONSUMING!

if(opts.use_optimal_step_size)
    opts.use_optimal_step_size =0;
    opts.step_parameter = optimal_step_size(x0, f_eval, g_eval, Hess_opt, boot_method, descent_method,   opts);
end
%%
x = x0;
f_full = @(x) f_eval(x, 1:opts.numdata);
f0 = f_full(x0);
f = f0;
iteration =1;
total_memory= 0; cur_time =0; xdiff =0;
if opts.prnt
    fprintf('%s \n', DATA.name);
    print_iteration_info;
end
% % Information for plotting
if(opts.plotting)
    errors = f0;%1;
    times = 0;
    DATA.datapasses = 0;
    DATA.datapasses_products =0;
end

while (iteration <= opts.max_iterations)
    tic; % START timing
    % Sample gradient
    if(strcmp(opts.grad_type,'SGD'))
        step_SGD;
    else
        step_SVRG;
    end
    % Calculate search direction
    d = descent_method(x, g_eval, Hess_opt ,iteration, opts);
    cur_time =cur_time+ toc;  % END Timing iteration
    if(strcmp(opts.grad_type,'SGD'))
        d = (opts.beta/(1+(iteration-1)*opts.alpha))*d;
    else
        d = opts.step_parameter*d;
    end
    x = x+d;
    xdiff = norm(d);
    if(mod(iteration,opts.inner_iterations)==1)
        f =  f_full(x);%1-accurary_prediction( opts.X, opts.y, x); %f_full(x);
        % Information for plotting
        
        if(isnan(f) || isinf(f))
            out.stopping_flag = 'NaN';
            break;
        end
        if(opts.plotting)
            errors(end+1) = f;% f/f0;
            times(end+1) = cur_time;
        end
        if opts.prnt
            if(isfield(opts,'hess_vec'))
                total_memory = total_memory+opts.hess_vec;
            end
            print_iteration_info;
        end
    end
    % ------------%
    iteration = iteration+1;
    if(cur_time > opts.Timeout)
        break;
    end
    if( DATA.datapasses(end) > opts.totalpasses)
        break;
    end
end % End of solver
%% Wrap up output
% Time the finish, remainder is for testing
out.cput = toc;
if (cur_time > opts.Timeout)
    out.stopping_flag = 'timeout';
elseif (isnan(f) || isnan(xdiff) || isinf(f) || isinf(xdiff))
    out.stopping_flag = 'NaN';
else
    out.stopping_flag = 'conv.';
end

out.name = DATA.name;
out.iteration = iteration;
out.x = x;
out.f = f;

if(opts.plotting)
    out.times =times;
    out.errors =errors;
    out.datapasses =DATA.datapasses;
end
%% Test the output
out.accuracy=  accurary_prediction( opts.X, opts.y, x);
%   out.val_accuracy = test_log_model(opts, out);
opts.X = [];
opts.y = [];
opts.x0 = [];
out.opts = opts;

%% Print conclusion
if opts.prnt
    iteration  =0;
    if(strcmp(opts.grad_type,'SGD'))
        fprintf('\n(beta,alpha) = (%f, %f)\n', opts.beta,opts.alpha );
    end
    print_iteration_info;
end
fprintf('%s total time %12.6f s', out.name,cur_time); display('   ');
end
